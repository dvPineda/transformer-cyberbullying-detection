{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-25 02:17:25.373629: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-25 02:17:25.409958: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-25 02:17:25.984088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = models.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset file from: J. Wang, K. Fu, C.T. Lu, “SOSNet: A Graph Convolutional Network Approach to Fine-Grained Cyberbullying Detection,” Proceedings of the 2020 IEEE International Conference on Big Data (IEEE BigData 2020), December 10-13, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          tweet_text cyberbullying_type\n",
      "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
      "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
      "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
      "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
      "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying\n",
      "\n",
      "After encoding labels:\n",
      "                                          tweet_text cyberbullying_type  label\n",
      "0  In other words #katandandre, your food was cra...  not_cyberbullying      0\n",
      "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying      0\n",
      "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying      0\n",
      "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying      0\n",
      "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying      0\n"
     ]
    }
   ],
   "source": [
    "# load cyberbullying dataset\n",
    "tweets_data = pd.read_csv('data/cyberbullying_tweets.csv')\n",
    "print(tweets_data.head())\n",
    "\n",
    "# encode labels\n",
    "tweets_data['label'] = tweets_data['cyberbullying_type'].factorize()[0]\n",
    "print(f'\\nAfter encoding labels:\\n{tweets_data.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    tweets_data['tweet_text'], \n",
    "    tweets_data['label'], \n",
    "    test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In other words #katandandre, your food was crapilicious! #mkr, 0\n"
     ]
    }
   ],
   "source": [
    "print('{0}, {1}'.format(x_train[0], y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "vocab_size = 20000\n",
    "\n",
    "# tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(x_train) # sets up the internal vocabulary using the training data\n",
    "x_train = tokenizer.texts_to_sequences(x_train) # convert text to sequence, i.e. each word is represented by a number\n",
    "x_val = tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,    40, 14912,   529,   143,\n",
       "         277,    27,    95,   113,  1144,    42,   628,  1346,     8,\n",
       "          87,   775, 14913,   290,  5413,  1125,   190, 14914,    85,\n",
       "         193], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-25 02:17:27.814033: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2024-05-25 02:17:27.814058: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:134] retrieving CUDA diagnostic information for host: vito-msi\n",
      "2024-05-25 02:17:27.814064: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:141] hostname: vito-msi\n",
      "2024-05-25 02:17:27.814181: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:165] libcuda reported version is: 550.54.15\n",
      "2024-05-25 02:17:27.814195: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:169] kernel reported version is: 550.54.15\n",
      "2024-05-25 02:17:27.814198: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:248] kernel version seems to match DSO: 550.54.15\n"
     ]
    }
   ],
   "source": [
    "# model construction\n",
    "embed_dim = 100 # embedding size for each token\n",
    "num_heads = 3 # number of attention heads\n",
    "ff_dim = 128 # hidden layer size in feed forward network inside transformer\n",
    "\n",
    "num_classes = len(tweets_data['cyberbullying_type'].unique())\n",
    "inputs = layers.Input(shape=(embed_dim,))\n",
    "embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n",
    "x = TransformerBlock(embed_dim, num_heads, ff_dim)(embedding_layer)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(50, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1193/1193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 29ms/step - accuracy: 0.3901 - loss: 1.4546 - val_accuracy: 0.7667 - val_loss: 0.4877\n",
      "Epoch 2/10\n",
      "\u001b[1m1193/1193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 49ms/step - accuracy: 0.7850 - loss: 0.4720 - val_accuracy: 0.8165 - val_loss: 0.4441\n",
      "Epoch 3/10\n",
      "\u001b[1m1193/1193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 81ms/step - accuracy: 0.8407 - loss: 0.3811 - val_accuracy: 0.8374 - val_loss: 0.3957\n",
      "Epoch 4/10\n",
      "\u001b[1m1193/1193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 126ms/step - accuracy: 0.8755 - loss: 0.3104 - val_accuracy: 0.8236 - val_loss: 0.4212\n",
      "Epoch 5/10\n",
      "\u001b[1m1193/1193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 57ms/step - accuracy: 0.8980 - loss: 0.2654 - val_accuracy: 0.8430 - val_loss: 0.4276\n",
      "Epoch 6/10\n",
      "\u001b[1m1193/1193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 48ms/step - accuracy: 0.9114 - loss: 0.2245 - val_accuracy: 0.8382 - val_loss: 0.4679\n",
      "Epoch 7/10\n",
      "\u001b[1m1193/1193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 43ms/step - accuracy: 0.9227 - loss: 0.1883 - val_accuracy: 0.8266 - val_loss: 0.5110\n",
      "Epoch 8/10\n",
      "\u001b[1m1193/1193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 48ms/step - accuracy: 0.9343 - loss: 0.1599 - val_accuracy: 0.8265 - val_loss: 0.5588\n",
      "Epoch 9/10\n",
      "\u001b[1m1193/1193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 48ms/step - accuracy: 0.9384 - loss: 0.1495 - val_accuracy: 0.8165 - val_loss: 0.6484\n",
      "Epoch 10/10\n",
      "\u001b[1m1193/1193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 49ms/step - accuracy: 0.9436 - loss: 0.1304 - val_accuracy: 0.8169 - val_loss: 0.6960\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001), \n",
    "    loss=\"sparse_categorical_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pr_transformer_env",
   "language": "python",
   "name": "pr_transformer_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
